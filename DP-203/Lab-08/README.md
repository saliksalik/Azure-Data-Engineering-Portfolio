# Lab 08: Integrate data from notebooks with Azure Data Factory or Azure Synapse Pipelines

This document outlines the steps I followed to integrate data from notebooks using Azure Data Factory or Azure Synapse Pipelines. Below are the detailed notes for each step.

## Lab Setup and Pre-requisites

Before starting, I ensured that Lab 6 was completed. If not, I followed the lab setup instructions to create the required linked services and datasets. Hereâ€™s what I did:

1. In Synapse Studio, I navigated to the Manage hub and added a new Linked Service for Azure Cosmos DB (SQL API) with the following settings:
   - Name: asacosmosdb01
   - Cosmos DB account name: asacosmosdbxxxxxxx
   - Database name: CustomerProfile

2. On the Data hub, I created the following Integration datasets:
   - `asal400_customerprofile_cosmosdb`:
     - Source: Azure Cosmos DB (SQL API)
     - Collection: OnlineUserProfile01
   - `asal400_ecommerce_userprofiles_source`:
     - Source: Azure Data Lake Storage Gen2
     - Format: JSON
     - File path: wwi-02/online-user-profiles-02

I noted that certain setup warnings could be ignored, such as errors related to null arrays.

## Exercise 1: Create mapping data flow and pipeline

### Task 1: Create mapping data flow

1. Opened Synapse Studio and navigated to the Develop hub.
2. Created a new data flow and named it `user_profiles_to_datalake`.
3. Replaced the default code with the provided JSON, ensuring the correct suffix was used for Azure resources.
4. Saved the data flow and verified its configuration.

### Task 2: Create pipeline

1. Created a new integration pipeline named `User Profiles to Datalake`.
2. Added a Data Flow activity to the pipeline canvas and configured it to use the `user_profiles_to_datalake` data flow.
3. Published the pipeline to save the changes.

### Task 3: Trigger the pipeline

1. Triggered the pipeline manually and monitored its execution in the Monitor hub.
2. Waited for the pipeline run to complete successfully.

## Exercise 2: Create Synapse Spark notebook to find top products

### Task 1: Create notebook

1. Navigated to the Data hub and loaded data from the `top-products` folder into a new notebook.
2. Named the notebook `Calculate Top 5 Products` and attached it to the `SparkPool01` Spark pool.
3. Updated the file path to include all Parquet files in the folder and ran the notebook.
4. Created additional code cells to process the data and calculate the top 5 products for each user and overall.

### Task 2: Add the Notebook to the pipeline

1. Added the notebook to the existing `User Profiles to Datalake` pipeline as a new Notebook activity.
2. Configured the Notebook activity to use the `runId` parameter, dynamically set to the pipeline run ID.
3. Published the updated pipeline.

### Task 3: Run the updated pipeline

1. Triggered the updated pipeline and monitored its execution.
2. Verified that the Parquet file generated by the notebook matched the pipeline run ID.

---

*These notes summarize the steps I followed to complete Lab 8 as part of the Azure Data Engineering Portfolio. All tasks were executed successfully.*
